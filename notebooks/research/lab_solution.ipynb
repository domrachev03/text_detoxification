{"cells":[{"cell_type":"markdown","metadata":{"id":"rEJBSTyZIrIb"},"source":["# Practical machine learning and deep learning. Lab 5\n","## Competition\n","No competition for today\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-tuning a model on a translation task\n","Today we will be finetunning T5(Text-To-Text Transfer Transformer) [model](https://github.com/google-research/t5x) on translation task. For this purpose we will be using [HuggingFace transformers](https://huggingface.co/docs/transformers/index) and [WMT16](https://huggingface.co/datasets/wmt16) dataset. "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2023-10-15T14:20:19.803044Z","iopub.status.busy":"2023-10-15T14:20:19.802221Z","iopub.status.idle":"2023-10-15T14:20:37.483548Z","shell.execute_reply":"2023-10-15T14:20:37.482436Z","shell.execute_reply.started":"2023-10-15T14:20:19.803010Z"},"id":"MOsHUjgdIrIW","outputId":"f84a093e-147f-470e-aad9-80fb51193c8e","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n","Requirement already satisfied: transformers[sentencepiece] in /opt/conda/lib/python3.10/site-packages (4.33.0)\n","Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.3.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.9.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.12.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (2023.6.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.3.3)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.1.99)\n","Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.20.3)\n","Requirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2.8.2)\n","Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n","Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.3)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: numpy==1.24.3 in /opt/conda/lib/python3.10/site-packages (1.24.3)\n"]}],"source":["# installing huggingface libraries for dataset, models and metrics\n","!pip install datasets transformers[sentencepiece] sacrebleu\n","\n","!pip install numpy==1.24.3"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:20:37.485880Z","iopub.status.busy":"2023-10-15T14:20:37.485516Z","iopub.status.idle":"2023-10-15T14:20:39.331594Z","shell.execute_reply":"2023-10-15T14:20:39.330698Z","shell.execute_reply.started":"2023-10-15T14:20:37.485843Z"},"trusted":true},"outputs":[],"source":["# Necessary inputs\n","import warnings\n","\n","from datasets import load_dataset, load_metric\n","import transformers\n","import datasets\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["## Selecting the model\n","For the example purpose we select as model checkpoint the smallest transformer in T5 family - `t5_small`. Other pre-trained models can be found [here](https://huggingface.co/docs/transformers/model_doc/t5#:~:text=T5%20comes%20in%20different%20sizes%3A)."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:20:39.333721Z","iopub.status.busy":"2023-10-15T14:20:39.333004Z","iopub.status.idle":"2023-10-15T14:20:39.337709Z","shell.execute_reply":"2023-10-15T14:20:39.336758Z","shell.execute_reply.started":"2023-10-15T14:20:39.333688Z"},"trusted":true},"outputs":[],"source":["# selecting model checkpoint\n","model_checkpoint = \"t5-small\""]},{"cell_type":"markdown","metadata":{"id":"whPRbBNbIrIl"},"source":["## Loading the dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:20:39.341610Z","iopub.status.busy":"2023-10-15T14:20:39.340830Z","iopub.status.idle":"2023-10-15T14:20:49.015577Z","shell.execute_reply":"2023-10-15T14:20:49.014638Z","shell.execute_reply.started":"2023-10-15T14:20:39.341571Z"},"id":"IreSlFmlIrIm","trusted":true},"outputs":[],"source":["# setting random seed for transformers library\n","transformers.set_seed(42)\n","\n","# Load the WMT16 dataset\n","df = pd.read_csv(\"/kaggle/input/toxic-comments-classification/filtered.tsv\", sep='\\t', index_col=0)\n","dataset = datasets.Dataset.from_pandas(df, split='train')\n","\n","# Load the BLUE metric\n","metric = load_metric(\"sacrebleu\")"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset\n","Downloaded from HuggingFace dataset is a `DatasetDict`. It contains keys `[\"train\", \"validation\", \"test\"]` - which represents a dataset splits"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:20:49.017532Z","iopub.status.busy":"2023-10-15T14:20:49.016857Z","iopub.status.idle":"2023-10-15T14:20:49.035347Z","shell.execute_reply":"2023-10-15T14:20:49.034362Z","shell.execute_reply.started":"2023-10-15T14:20:49.017496Z"},"id":"GWiVUF0jIrIv","outputId":"35e3ea43-f397-4a54-c90c-f2cf8d36873e","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reference</th>\n","      <th>translation</th>\n","      <th>similarity</th>\n","      <th>lenght_diff</th>\n","      <th>ref_tox</th>\n","      <th>trn_tox</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>If Alkar is flooding her with psychic waste, t...</td>\n","      <td>if Alkar floods her with her mental waste, it ...</td>\n","      <td>0.785171</td>\n","      <td>0.010309</td>\n","      <td>0.014195</td>\n","      <td>0.981983</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Now you're getting nasty.</td>\n","      <td>you're becoming disgusting.</td>\n","      <td>0.749687</td>\n","      <td>0.071429</td>\n","      <td>0.065473</td>\n","      <td>0.999039</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Well, we could spare your life, for one.</td>\n","      <td>well, we can spare your life.</td>\n","      <td>0.919051</td>\n","      <td>0.268293</td>\n","      <td>0.213313</td>\n","      <td>0.985068</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Ah! Monkey, you've got to snap out of it.</td>\n","      <td>monkey, you have to wake up.</td>\n","      <td>0.664333</td>\n","      <td>0.309524</td>\n","      <td>0.053362</td>\n","      <td>0.994215</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I've got orders to put her down.</td>\n","      <td>I have orders to kill her.</td>\n","      <td>0.726639</td>\n","      <td>0.181818</td>\n","      <td>0.009402</td>\n","      <td>0.999348</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           reference  \\\n","0  If Alkar is flooding her with psychic waste, t...   \n","1                          Now you're getting nasty.   \n","2           Well, we could spare your life, for one.   \n","3          Ah! Monkey, you've got to snap out of it.   \n","4                   I've got orders to put her down.   \n","\n","                                         translation  similarity  lenght_diff  \\\n","0  if Alkar floods her with her mental waste, it ...    0.785171     0.010309   \n","1                        you're becoming disgusting.    0.749687     0.071429   \n","2                      well, we can spare your life.    0.919051     0.268293   \n","3                       monkey, you have to wake up.    0.664333     0.309524   \n","4                         I have orders to kill her.    0.726639     0.181818   \n","\n","    ref_tox   trn_tox  \n","0  0.014195  0.981983  \n","1  0.065473  0.999039  \n","2  0.213313  0.985068  \n","3  0.053362  0.994215  \n","4  0.009402  0.999348  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:20:49.037497Z","iopub.status.busy":"2023-10-15T14:20:49.036596Z","iopub.status.idle":"2023-10-15T14:20:49.254796Z","shell.execute_reply":"2023-10-15T14:20:49.253643Z","shell.execute_reply.started":"2023-10-15T14:20:49.037466Z"},"trusted":true},"outputs":[],"source":["# Swap translation based on the toxicity\n","cond = (df[\"ref_tox\"] < df[\"trn_tox\"])\n","df.loc[cond, ['reference', 'translation']] = (\n","    df.loc[cond, ['translation', 'reference']].values)\n","df.loc[cond, ['ref_tox', 'trn_tox']] = (\n","    df.loc[cond, ['trn_tox', 'ref_tox']].values)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:20:49.256628Z","iopub.status.busy":"2023-10-15T14:20:49.256251Z","iopub.status.idle":"2023-10-15T14:20:49.270901Z","shell.execute_reply":"2023-10-15T14:20:49.269883Z","shell.execute_reply.started":"2023-10-15T14:20:49.256592Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reference</th>\n","      <th>translation</th>\n","      <th>similarity</th>\n","      <th>lenght_diff</th>\n","      <th>ref_tox</th>\n","      <th>trn_tox</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>if Alkar floods her with her mental waste, it ...</td>\n","      <td>If Alkar is flooding her with psychic waste, t...</td>\n","      <td>0.785171</td>\n","      <td>0.010309</td>\n","      <td>0.981983</td>\n","      <td>0.014195</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>you're becoming disgusting.</td>\n","      <td>Now you're getting nasty.</td>\n","      <td>0.749687</td>\n","      <td>0.071429</td>\n","      <td>0.999039</td>\n","      <td>0.065473</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>well, we can spare your life.</td>\n","      <td>Well, we could spare your life, for one.</td>\n","      <td>0.919051</td>\n","      <td>0.268293</td>\n","      <td>0.985068</td>\n","      <td>0.213313</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>monkey, you have to wake up.</td>\n","      <td>Ah! Monkey, you've got to snap out of it.</td>\n","      <td>0.664333</td>\n","      <td>0.309524</td>\n","      <td>0.994215</td>\n","      <td>0.053362</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I have orders to kill her.</td>\n","      <td>I've got orders to put her down.</td>\n","      <td>0.726639</td>\n","      <td>0.181818</td>\n","      <td>0.999348</td>\n","      <td>0.009402</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>577772</th>\n","      <td>you didn't know that Estelle stole your fish f...</td>\n","      <td>You didn't know that Estelle had stolen some f...</td>\n","      <td>0.870322</td>\n","      <td>0.030769</td>\n","      <td>0.949143</td>\n","      <td>0.000121</td>\n","    </tr>\n","    <tr>\n","      <th>577773</th>\n","      <td>It'il suck the life out of you!</td>\n","      <td>you'd be sucked out of your life!</td>\n","      <td>0.722897</td>\n","      <td>0.058824</td>\n","      <td>0.996124</td>\n","      <td>0.215794</td>\n","    </tr>\n","    <tr>\n","      <th>577774</th>\n","      <td>I can't fuckin' take that, bruv.</td>\n","      <td>I really can't take this.</td>\n","      <td>0.617511</td>\n","      <td>0.212121</td>\n","      <td>0.984538</td>\n","      <td>0.000049</td>\n","    </tr>\n","    <tr>\n","      <th>577775</th>\n","      <td>They called me a fucking hero. The truth is I ...</td>\n","      <td>they said I was a hero, but I didn't care.</td>\n","      <td>0.679613</td>\n","      <td>0.358209</td>\n","      <td>0.991945</td>\n","      <td>0.000124</td>\n","    </tr>\n","    <tr>\n","      <th>577776</th>\n","      <td>I didn't fuck him.</td>\n","      <td>I did not screw him.</td>\n","      <td>0.868475</td>\n","      <td>0.095238</td>\n","      <td>0.994174</td>\n","      <td>0.009480</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>577777 rows × 6 columns</p>\n","</div>"],"text/plain":["                                                reference  \\\n","0       if Alkar floods her with her mental waste, it ...   \n","1                             you're becoming disgusting.   \n","2                           well, we can spare your life.   \n","3                            monkey, you have to wake up.   \n","4                              I have orders to kill her.   \n","...                                                   ...   \n","577772  you didn't know that Estelle stole your fish f...   \n","577773                    It'il suck the life out of you!   \n","577774                   I can't fuckin' take that, bruv.   \n","577775  They called me a fucking hero. The truth is I ...   \n","577776                                 I didn't fuck him.   \n","\n","                                              translation  similarity  \\\n","0       If Alkar is flooding her with psychic waste, t...    0.785171   \n","1                               Now you're getting nasty.    0.749687   \n","2                Well, we could spare your life, for one.    0.919051   \n","3               Ah! Monkey, you've got to snap out of it.    0.664333   \n","4                        I've got orders to put her down.    0.726639   \n","...                                                   ...         ...   \n","577772  You didn't know that Estelle had stolen some f...    0.870322   \n","577773                  you'd be sucked out of your life!    0.722897   \n","577774                          I really can't take this.    0.617511   \n","577775         they said I was a hero, but I didn't care.    0.679613   \n","577776                               I did not screw him.    0.868475   \n","\n","        lenght_diff   ref_tox   trn_tox  \n","0          0.010309  0.981983  0.014195  \n","1          0.071429  0.999039  0.065473  \n","2          0.268293  0.985068  0.213313  \n","3          0.309524  0.994215  0.053362  \n","4          0.181818  0.999348  0.009402  \n","...             ...       ...       ...  \n","577772     0.030769  0.949143  0.000121  \n","577773     0.058824  0.996124  0.215794  \n","577774     0.212121  0.984538  0.000049  \n","577775     0.358209  0.991945  0.000124  \n","577776     0.095238  0.994174  0.009480  \n","\n","[577777 rows x 6 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:49:13.230081Z","iopub.status.busy":"2023-10-15T14:49:13.229255Z","iopub.status.idle":"2023-10-15T14:49:13.268459Z","shell.execute_reply":"2023-10-15T14:49:13.267356Z","shell.execute_reply.started":"2023-10-15T14:49:13.230043Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["577777\n","173907\n"]}],"source":["print(len(df))\n","filtered_df = df[(df[\"ref_tox\"] > 0.99) & (df[\"trn_tox\"] < 0.01)]\n","\n","print(len(filtered_df))"]},{"cell_type":"code","execution_count":89,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:49:15.802121Z","iopub.status.busy":"2023-10-15T14:49:15.801755Z","iopub.status.idle":"2023-10-15T14:49:15.912010Z","shell.execute_reply":"2023-10-15T14:49:15.911048Z","shell.execute_reply.started":"2023-10-15T14:49:15.802091Z"},"trusted":true},"outputs":[],"source":["dataset = datasets.Dataset.from_pandas(filtered_df).remove_columns('__index_level_0__')\n","\n","split_dict = dataset.train_test_split(\n","    test_size=0.1,\n","    seed=42,\n",")"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:49:17.444209Z","iopub.status.busy":"2023-10-15T14:49:17.443857Z","iopub.status.idle":"2023-10-15T14:49:17.450218Z","shell.execute_reply":"2023-10-15T14:49:17.449191Z","shell.execute_reply.started":"2023-10-15T14:49:17.444179Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['reference', 'translation', 'similarity', 'lenght_diff', 'ref_tox', 'trn_tox'],\n","    num_rows: 17391\n","})"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["split_dict[\"test\"]"]},{"cell_type":"markdown","metadata":{},"source":["## Metric\n","[Sacrebleu](https://huggingface.co/spaces/evaluate-metric/sacrebleu) computes:\n","- `score`: BLEU score\n","- `counts`: list of counts of correct n-grams\n","- `totals`: list of counts of total n-grams\n","- `precisions`: list of precisions\n","- `bp`: Brevity penalty\n","- `sys_len`: cumulative sysem length\n","- `ref_len`: cumulative reference length\n","\n","The main metric is [BLEU score](https://en.wikipedia.org/wiki/BLEU). BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score measures the similarity of the machine-translated text to a set of high quality reference translations.\n","\n","The BLEU metric is calculates using [n-grams](https://en.wikipedia.org/wiki/N-gram)."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:20:49.562609Z","iopub.status.busy":"2023-10-15T14:20:49.561524Z","iopub.status.idle":"2023-10-15T14:20:49.589728Z","shell.execute_reply":"2023-10-15T14:20:49.588739Z","shell.execute_reply.started":"2023-10-15T14:20:49.562577Z"},"id":"6XN1Rq0aIrJC","outputId":"a4405435-a8a9-41ff-9f79-a13077b587c7","trusted":true},"outputs":[{"data":{"text/plain":["{'score': 45.59274666224604,\n"," 'counts': [7, 4, 1, 0],\n"," 'totals': [9, 6, 3, 2],\n"," 'precisions': [77.77777777777777,\n","  66.66666666666667,\n","  33.333333333333336,\n","  25.0],\n"," 'bp': 1.0,\n"," 'sys_len': 9,\n"," 'ref_len': 9}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["fake_preds = [\"hello there\", \"general kenobi\", \"Can I get an A\"]\n","fake_labels = [[\"hello there\"], [\"general kenobi\"], ['Can I get a C']]\n","metric.compute(predictions=fake_preds, references=fake_labels)"]},{"cell_type":"markdown","metadata":{"id":"n9qywopnIrJH"},"source":["## Preprocessing the data\n","As usual we will need to preprocess data and tokenize it before passing to model"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:20:49.591580Z","iopub.status.busy":"2023-10-15T14:20:49.591054Z","iopub.status.idle":"2023-10-15T14:20:50.298942Z","shell.execute_reply":"2023-10-15T14:20:50.297919Z","shell.execute_reply.started":"2023-10-15T14:20:49.591550Z"},"id":"eXNLu_-nIrJI","trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","# we will use autotokenizer for this purpose\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:20:50.303708Z","iopub.status.busy":"2023-10-15T14:20:50.303129Z","iopub.status.idle":"2023-10-15T14:20:50.312644Z","shell.execute_reply":"2023-10-15T14:20:50.311458Z","shell.execute_reply.started":"2023-10-15T14:20:50.303662Z"},"id":"a5hBlsrHIrJL","outputId":"acdaa98a-a8cd-4a20-89b8-cc26437bbe90","trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [8774, 6, 48, 80, 7142, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(\"Hello, this one sentence!\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:20:50.314709Z","iopub.status.busy":"2023-10-15T14:20:50.313890Z","iopub.status.idle":"2023-10-15T14:20:50.324268Z","shell.execute_reply":"2023-10-15T14:20:50.323183Z","shell.execute_reply.started":"2023-10-15T14:20:50.314677Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:20:50.326657Z","iopub.status.busy":"2023-10-15T14:20:50.325927Z","iopub.status.idle":"2023-10-15T14:20:50.331135Z","shell.execute_reply":"2023-10-15T14:20:50.330170Z","shell.execute_reply.started":"2023-10-15T14:20:50.326624Z"},"trusted":true},"outputs":[],"source":["# prefix for model input\n","prefix = \"Make this text non-toxic:\""]},{"cell_type":"code","execution_count":91,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:50:21.369429Z","iopub.status.busy":"2023-10-15T14:50:21.369049Z","iopub.status.idle":"2023-10-15T14:50:21.375186Z","shell.execute_reply":"2023-10-15T14:50:21.374187Z","shell.execute_reply.started":"2023-10-15T14:50:21.369388Z"},"id":"vc0BSBLIIrJQ","trusted":true},"outputs":[],"source":["max_input_length = 128\n","max_target_length = 128\n","\n","def preprocess_function(examples):\n","    inputs = [prefix + ref for ref in examples[\"reference\"]]\n","    targets = [tsn for tsn in examples[\"translation\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_overflowing_tokens=False)\n","\n","    # Setup the tokenizer for targets\n","    labels = tokenizer(targets, max_length=max_target_length, truncation=True, return_overflowing_tokens=False)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:50:22.960349Z","iopub.status.busy":"2023-10-15T14:50:22.959984Z","iopub.status.idle":"2023-10-15T14:50:22.970053Z","shell.execute_reply":"2023-10-15T14:50:22.968981Z","shell.execute_reply.started":"2023-10-15T14:50:22.960318Z"},"id":"-b70jh26IrJS","outputId":"acd3a42d-985b-44ee-9daa-af5d944ce1d9","trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [[1796, 48, 1499, 529, 18, 14367, 10, 10499, 32, 32, 17, 5, 9459, 6, 18117, 6, 43, 25, 894, 82, 1379, 570, 58, 1], [1796, 48, 1499, 529, 18, 14367, 10, 4067, 3, 27826, 19, 8, 833, 44, 160, 629, 469, 58, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[11604, 6, 43, 25, 894, 82, 1379, 570, 58, 1], [571, 6819, 19, 3, 9, 833, 44, 160, 629, 8988, 58, 1]]}"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["# example of preprocessing\n","preprocess_function(split_dict['train'][:2])"]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:53:14.652243Z","iopub.status.busy":"2023-10-15T14:53:14.651834Z","iopub.status.idle":"2023-10-15T14:53:15.403506Z","shell.execute_reply":"2023-10-15T14:53:15.402553Z","shell.execute_reply.started":"2023-10-15T14:53:14.652209Z"},"id":"DDtsaJeVIrJT","outputId":"aa4734bf-4ef5-4437-9948-2c16363da719","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b725ba22be34d8391e4b1f185e793ab","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/20 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5927d2d046c1484db88a91213f1d59fd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'input_ids': [1796,\n","  48,\n","  1499,\n","  529,\n","  18,\n","  14367,\n","  10,\n","  10499,\n","  32,\n","  32,\n","  17,\n","  5,\n","  9459,\n","  6,\n","  18117,\n","  6,\n","  43,\n","  25,\n","  894,\n","  82,\n","  1379,\n","  570,\n","  58,\n","  1],\n"," 'attention_mask': [1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1],\n"," 'labels': [11604, 6, 43, 25, 894, 82, 1379, 570, 58, 1]}"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["# for the example purpose we will crop the dataset and select first 5000 for train\n","# and 500 for validation and test\n","batch_size = 256\n","cropped_datasets = split_dict\n","cropped_datasets['train'] = split_dict['train'].select(range(5000))\n","cropped_datasets['test'] = split_dict['test'].select(range(500))\n","tokenized_datasets = cropped_datasets.map(preprocess_function, batched=True, batch_size=batch_size, remove_columns=split_dict[\"train\"].column_names)\n","tokenized_datasets['train'][0]"]},{"cell_type":"markdown","metadata":{"id":"545PP3o8IrJV"},"source":["## Fine-tuning the model"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:21:00.490768Z","iopub.status.busy":"2023-10-15T14:21:00.490096Z","iopub.status.idle":"2023-10-15T14:21:17.347857Z","shell.execute_reply":"2023-10-15T14:21:17.346698Z","shell.execute_reply.started":"2023-10-15T14:21:00.490734Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.0.314)\n","Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.17)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.4)\n","Requirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.7.0)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.2)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\n","Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.43)\n","Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.3)\n","Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.9)\n","Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.2)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain) (3.4)\n","Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.1.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.6.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n","Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n","Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (21.3)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (3.0.9)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (2.2.2)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.33.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\n","Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.0.0)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.24.3)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.2)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.16.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.9.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.6.3)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.3)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.16.0)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"]}],"source":["!pip install langchain\n","!pip install sentence-transformers"]},{"cell_type":"code","execution_count":103,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:53:24.567334Z","iopub.status.busy":"2023-10-15T14:53:24.566950Z","iopub.status.idle":"2023-10-15T14:53:25.921880Z","shell.execute_reply":"2023-10-15T14:53:25.920829Z","shell.execute_reply.started":"2023-10-15T14:53:24.567299Z"},"id":"TlqNaB8jIrJW","outputId":"84916cf3-6e6c-47f3-d081-032ec30a4132","trusted":true},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","# create a model for the pretrained model\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":104,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:53:25.924525Z","iopub.status.busy":"2023-10-15T14:53:25.923830Z","iopub.status.idle":"2023-10-15T14:53:25.931466Z","shell.execute_reply":"2023-10-15T14:53:25.930443Z","shell.execute_reply.started":"2023-10-15T14:53:25.924489Z"},"id":"Bliy8zgjIrJY","trusted":true},"outputs":[],"source":["# defining the parameters for training\n","model_name = model_checkpoint.split(\"/\")[-1]\n","args = Seq2SeqTrainingArguments(\n","    f\"{model_name}-finetuned-detoxification\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=10,\n","    predict_with_generate=True,\n","    fp16=True,\n","    report_to='tensorboard',\n",")"]},{"cell_type":"code","execution_count":105,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:53:25.933797Z","iopub.status.busy":"2023-10-15T14:53:25.933040Z","iopub.status.idle":"2023-10-15T14:53:25.940193Z","shell.execute_reply":"2023-10-15T14:53:25.939171Z","shell.execute_reply.started":"2023-10-15T14:53:25.933737Z"},"trusted":true},"outputs":[],"source":["# instead of writing collate_fn function we will use DataCollatorForSeq2Seq\n","# simliarly it implements the batch creation for training\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"]},{"cell_type":"code","execution_count":106,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:53:25.943163Z","iopub.status.busy":"2023-10-15T14:53:25.942384Z","iopub.status.idle":"2023-10-15T14:53:25.952057Z","shell.execute_reply":"2023-10-15T14:53:25.951067Z","shell.execute_reply.started":"2023-10-15T14:53:25.943130Z"},"id":"UmvbnJ9JIrJd","trusted":true},"outputs":[],"source":["import numpy as np\n","\n","# simple postprocessing for text\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [[label.strip()] for label in labels]\n","\n","    return preds, labels\n","\n","# compute metrics function to pass to trainer\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    \n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Some simple post-processing\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    result = {\"bleu\": result[\"score\"]}\n","\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    result = {k: round(v, 4) for k, v in result.items()}\n","    return result"]},{"cell_type":"code","execution_count":107,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:53:25.954436Z","iopub.status.busy":"2023-10-15T14:53:25.953414Z","iopub.status.idle":"2023-10-15T14:53:26.044448Z","shell.execute_reply":"2023-10-15T14:53:26.043510Z","shell.execute_reply.started":"2023-10-15T14:53:25.954402Z"},"id":"imY1oC3SIrJf","trusted":true},"outputs":[],"source":["# instead of writing train loop we will use Seq2SeqTrainer\n","trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","execution_count":108,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T14:53:26.046733Z","iopub.status.busy":"2023-10-15T14:53:26.045867Z","iopub.status.idle":"2023-10-15T15:37:18.509555Z","shell.execute_reply":"2023-10-15T15:37:18.508103Z","shell.execute_reply.started":"2023-10-15T14:53:26.046697Z"},"id":"uNx5pyRlIrJh","outputId":"077e661e-d36c-469b-89b8-7ff7f73541ec","trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='30018' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [30018/50000 43:51 < 29:11, 11.41 it/s, Epoch 6.00/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.035100</td>\n","      <td>1.863859</td>\n","      <td>22.931200</td>\n","      <td>12.372000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.956400</td>\n","      <td>1.801350</td>\n","      <td>23.737300</td>\n","      <td>12.446000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.876500</td>\n","      <td>1.771070</td>\n","      <td>24.465900</td>\n","      <td>12.270000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.771400</td>\n","      <td>1.773111</td>\n","      <td>24.758700</td>\n","      <td>12.196000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.709300</td>\n","      <td>1.764896</td>\n","      <td>24.878900</td>\n","      <td>12.218000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>1.665300</td>\n","      <td>1.766890</td>\n","      <td>25.301200</td>\n","      <td>12.102000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[108], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1835\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1835\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1838\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1840\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1841\u001b[0m ):\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2690\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2688\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2690\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:1921\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1921\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1923\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":109,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T15:37:32.179027Z","iopub.status.busy":"2023-10-15T15:37:32.178642Z","iopub.status.idle":"2023-10-15T15:37:33.072577Z","shell.execute_reply":"2023-10-15T15:37:33.071746Z","shell.execute_reply.started":"2023-10-15T15:37:32.178993Z"},"trusted":true},"outputs":[],"source":["# saving model\n","trainer.save_model('best')"]},{"cell_type":"code","execution_count":110,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T15:37:34.083314Z","iopub.status.busy":"2023-10-15T15:37:34.082148Z","iopub.status.idle":"2023-10-15T15:37:35.270865Z","shell.execute_reply":"2023-10-15T15:37:35.269856Z","shell.execute_reply.started":"2023-10-15T15:37:34.083254Z"},"trusted":true},"outputs":[],"source":["# loading the model and run inference for it\n","model = AutoModelForSeq2SeqLM.from_pretrained('best')\n","model.eval()\n","model.config.use_cache = False"]},{"cell_type":"code","execution_count":111,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T15:37:36.859435Z","iopub.status.busy":"2023-10-15T15:37:36.858270Z","iopub.status.idle":"2023-10-15T15:37:36.865702Z","shell.execute_reply":"2023-10-15T15:37:36.864241Z","shell.execute_reply.started":"2023-10-15T15:37:36.859386Z"},"trusted":true},"outputs":[],"source":["def detoxify(model, inference_request, tokenizer=tokenizer):\n","    input_ids = tokenizer(inference_request, return_tensors=\"pt\").input_ids\n","    outputs = model.generate(input_ids=input_ids)\n","    print(tokenizer.decode(outputs[0], skip_special_tokens=True,temperature=0))"]},{"cell_type":"code","execution_count":117,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T15:38:18.056463Z","iopub.status.busy":"2023-10-15T15:38:18.056064Z","iopub.status.idle":"2023-10-15T15:38:18.372658Z","shell.execute_reply":"2023-10-15T15:38:18.371627Z","shell.execute_reply.started":"2023-10-15T15:38:18.056432Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["you're gonna be a mess!\n"]}],"source":["inference_request = prefix +\"Fuck you, bitch!\"\n","detoxify(model, inference_request,tokenizer)"]},{"cell_type":"code","execution_count":113,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T15:37:44.060097Z","iopub.status.busy":"2023-10-15T15:37:44.059743Z","iopub.status.idle":"2023-10-15T15:37:44.455484Z","shell.execute_reply":"2023-10-15T15:37:44.454434Z","shell.execute_reply.started":"2023-10-15T15:37:44.060066Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["this man could not even have tensors in mind!\n"]}],"source":["inference_request = prefix + 'This bastard could not even mutliply fucking tensors in mind!'\n","detoxify(model, inference_request,tokenizer)"]},{"cell_type":"code","execution_count":118,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T15:39:06.151556Z","iopub.status.busy":"2023-10-15T15:39:06.151136Z","iopub.status.idle":"2023-10-15T15:39:06.678780Z","shell.execute_reply":"2023-10-15T15:39:06.677761Z","shell.execute_reply.started":"2023-10-15T15:39:06.151520Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["you sound like a sailor, sailor, sailor\n"]}],"source":["inference_request = prefix + \"\"\"You sound like a bitch, bitch\n","Shut the fuck up\n","When your fans become your haters\n","You done?\n","Fuck, your beard's weird\n","Alright\n","You yellin' at the mic, you weird beard\n","We doin' this once\n","Your beard's weird, why you yellin' at the mic?\"\"\"\n","detoxify(model, inference_request,tokenizer)"]}],"metadata":{"colab":{"name":"Translation","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
