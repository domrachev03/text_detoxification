{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Llama2 7b tuning & inference"]},{"cell_type":"markdown","metadata":{},"source":["So, what if we try something... bigger?...\n","\n","This contestant is out of league for several reasons:\n","1. It has much more parameters than other models (60 and 124M vs 7b). It would be much more appropriate to compare it with heavier versions of GPT2 and T5, however I do not have enough resources for that ðŸ™ƒ\n","2. This model takes way more time to infere than lighter models (for obvious reasons)\n","\n","Why then I chosed this model as an option? Well, Llama2 is one of the most powerful publicly available transformers at the moment, and I consider skills I acuired while working with it quite handy."]},{"cell_type":"markdown","metadata":{},"source":["## Llama2 7b tuning"]},{"cell_type":"markdown","metadata":{},"source":["Obviously, I could not do it easily either on the kaggle or locally, because I do not have enough resources available for that. That's why I decided to use service called [modal.com](https://modal.com/) and utilize their computational power to run the evaluations.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Reproduction steps"]},{"cell_type":"markdown","metadata":{},"source":["First of all, you have to gain access to the service:\n","1. Regiseter in [modal.com](https://modal.com/) (1 minute, requires GitHub authentication)\n","2. Enter secret from Huggingface (enter the hf token in the `HUGGINGFACE_TOKEN` field and name it `huggingface`), which could be found in the `Settings/API tokens`.\n"," \n","The tool is much easier to use via the terminal, because it generates way too much output. Here is the list of commands to launch it in CLI (and corresponding cell with these commands):\n","```bash\n","# Authorization in modal account\n","modal token new   \n","# Launch training process\n","modal run src/models/llama/train_modal.py --dataset llama2_dataset.py --base chat7 --run-id chat7-nontoxic\n","# Copying PEFT pretrained model from modal cloud to local dir\n","modal volume get example-results-vol 'chat7-nontoxic/*' models/llama2 \n","# Running inference for the model in cloud\n","modal run inference.py --base chat7 --run-id chat7-nontoxic --prompt \"[INST]<<SYS>>\\nYou are a Twitch moderator that paraphrases sentences to be non-toxic.\\n<<SYS>> \\n\\nCould you paraphrase this: ...?\\n [/INST]\"\n","\n","```"]},{"cell_type":"markdown","metadata":{},"source":["The implementation of inference with model running locally is represented below"]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T16:27:19.779479Z","iopub.status.busy":"2023-11-02T16:27:19.779115Z","iopub.status.idle":"2023-11-02T16:27:59.452893Z","shell.execute_reply":"2023-11-02T16:27:59.451598Z","shell.execute_reply.started":"2023-11-02T16:27:19.779438Z"},"trusted":true},"outputs":[],"source":["!pip install -q peft\n","!pip install -q --upgrade bitsandbytes\n","!pip install -q --upgrade accelerate\n","!pip install -q sacrebleu\n","!pip install -q evaluate"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-02T16:27:59.459010Z","iopub.status.busy":"2023-11-02T16:27:59.458641Z","iopub.status.idle":"2023-11-02T16:28:12.627645Z","shell.execute_reply":"2023-11-02T16:28:12.626817Z","shell.execute_reply.started":"2023-11-02T16:27:59.458984Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","from collections.abc import Iterable\n","from tqdm.auto import trange\n","import torch\n","import numpy as np\n","import peft\n","import transformers, accelerate, bitsandbytes\n","\n","import gc\n","import tqdm\n","from tqdm.auto import trange\n","import torch\n","import numpy as np\n","\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, \\\n","    RobertaTokenizer, RobertaForSequenceClassification\n","\n","import evaluate\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["The functions for wrapping the message and running the inference. Note, that we are loading quantified model, since the resources of the Colab is not enough to run it without compression."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T16:28:12.629363Z","iopub.status.busy":"2023-11-02T16:28:12.628779Z","iopub.status.idle":"2023-11-02T16:28:12.640899Z","shell.execute_reply":"2023-11-02T16:28:12.639997Z","shell.execute_reply.started":"2023-11-02T16:28:12.629333Z"},"trusted":true},"outputs":[],"source":["def wrap_messages(msgs):\n","    B_INST, E_INST = \"[INST] \", \" [/INST]\"\n","    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n","    prefixed_queries = [\n","        B_INST\n","        + B_SYS\n","        + \"You are a Twitch moderator that paraphrases sentences to be non-toxic.\\n\"\n","        + E_SYS\n","        + \"Could you paraphrase this: \"\n","        + msg\n","        + \"?\\n\"\n","        + E_INST\n","        for msg in msgs\n","    ]\n","    return prefixed_queries\n","\n","\n","def predict(requests, greb_answer = False, batch_size = 1, max_length = 64):\n","    requests = wrap_messages(requests)\n","    \n","    model = AutoModelForCausalLM.from_pretrained(\n","        'daryl149/llama-2-7b-chat-hf', \n","        load_in_4bit=True, \n","        bnb_4bit_compute_dtype=torch.float16\n","    )\n","    model.load_adapter('domrachev03/llama2_7b_detoxification')\n","    model.eval()\n","        \n","    tokenizer = AutoTokenizer.from_pretrained('daryl149/llama-2-7b-chat-hf')\n","    tokenizer.pad_token = tokenizer.eos_token\n","    \n","    \n","    results = []\n","    for i in trange(0, len(requests), batch_size):\n","        batch = [t for t in requests[i: i + batch_size]]\n","        inputs = tokenizer(\n","            batch, \n","            padding=True, \n","            truncation=True, \n","            max_length = max_length, \n","            return_tensors='pt'\n","        ).input_ids.to(model.device)\n","        \n","        with torch.no_grad():\n","            out = model.generate(inputs, max_new_tokens=max_length+1)\n","            decoded = [tokenizer.decode(out_i, skip_special_tokens=True,temperature=0) for out_i in out]\n","            \n","            if greb_answer:\n","                decoded = [\n","                    decoded[k][decoded[k].find('[/INST]')+len('[/INST]') : decoded[k].find('</s>')] \n","                    for k in range(len(decoded))\n","                ]\n","            results.extend(decoded)\n","    \n","    return results"]},{"cell_type":"markdown","metadata":{},"source":["Test launch"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-02T16:28:12.644665Z","iopub.status.busy":"2023-11-02T16:28:12.644397Z","iopub.status.idle":"2023-11-02T16:30:28.398625Z","shell.execute_reply":"2023-11-02T16:30:28.397471Z","shell.execute_reply.started":"2023-11-02T16:28:12.644641Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37875174bba84c25931b78384d0884bf","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aeb461c971b3463b9ea6aaf9045067e0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[\"you know, I'm sorry, but I can't do that.\",\n"," 'this chair is driving me crazy.',\n"," 'this sauce, I love it.',\n"," 'I hate gays.',\n"," \"I'm just a little puppy.\"]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["queries = ['Fuck you!', 'This freaking chair makes me nuts', 'This fucking sause, I love it', 'I hate gays', 'Pupupu']\n","\n","predict(queries, greb_answer=True, batch_size=2)"]},{"cell_type":"markdown","metadata":{},"source":["## Computing the results"]},{"cell_type":"markdown","metadata":{},"source":["Now, let's load the test dataset and check the performance of the model. Note, that the inference on the whole test dataset would take too much time, and hence only a fraction of it is utilized"]},{"cell_type":"markdown","metadata":{},"source":["> Note: The current setup utilizes 20Gb of RAM and 15.9Gb of videomemory. This is barely enough to run on `Nvidia P100` in Kaggle. "]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T16:30:28.400140Z","iopub.status.busy":"2023-11-02T16:30:28.399832Z","iopub.status.idle":"2023-11-02T16:30:29.421848Z","shell.execute_reply":"2023-11-02T16:30:29.421067Z","shell.execute_reply.started":"2023-11-02T16:30:28.400111Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9cd6a7678fc548e79d83c5de98e9e107","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import datasets\n","\n","dataset = datasets.load_dataset(\"domrachev03/toxic_comments_subset\")\n","test_subset = dataset['test'].select(range(5000))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T16:30:29.424135Z","iopub.status.busy":"2023-11-02T16:30:29.423014Z","iopub.status.idle":"2023-11-02T17:03:07.146742Z","shell.execute_reply":"2023-11-02T17:03:07.145550Z","shell.execute_reply.started":"2023-11-02T16:30:29.424082Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca308c20d01a42d9a4c1bffc64c73f2c","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8fd5995457a24338a1ffe0e17350261c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/79 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["test_preds = predict([*test_subset['reference']], greb_answer=True, batch_size=64)"]},{"cell_type":"markdown","metadata":{},"source":["## Metrics & saving"]},{"cell_type":"markdown","metadata":{},"source":["Now, let's compute the metrics for the model "]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T17:08:10.678534Z","iopub.status.busy":"2023-11-02T17:08:10.677439Z","iopub.status.idle":"2023-11-02T17:08:10.700818Z","shell.execute_reply":"2023-11-02T17:08:10.699687Z","shell.execute_reply.started":"2023-11-02T17:08:10.678492Z"},"trusted":true},"outputs":[],"source":["def cleanup():\n","    gc.collect()\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","\n","\n","def get_toxicity(preds, soft=False, batch_size=1, device='cuda'):\n","    results = []\n","\n","    model_name = 'SkolkovoInstitute/roberta_toxicity_classifier'\n","\n","    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","    model = RobertaForSequenceClassification.from_pretrained(model_name)\n","    device = device\n","    model.to(device)\n","\n","    model.eval()\n","    for i in tqdm.tqdm(range(0, len(preds), batch_size)):\n","        batch = tokenizer(preds[i:i + batch_size], return_tensors='pt', max_length=-1, padding=True).to(device)\n","\n","        with torch.no_grad():\n","            logits = model(**batch).logits\n","            out = torch.softmax(logits, -1)[:, 1].cpu().numpy()\n","            results.append(out)\n","    return 1 - np.concatenate(results)\n","\n","\n","def get_sacrebleu(inputs, preds):\n","    metric = evaluate.load(\"sacrebleu\")\n","\n","    result = metric.compute(predictions=preds, references=inputs)\n","    return result['score']\n","\n","\n","def get_fluency(preds, soft=False, batch_size=1, device='cuda'):\n","    path = 'cointegrated/roberta-large-cola-krishna2020'\n","\n","    model = RobertaForSequenceClassification.from_pretrained(path)\n","    tokenizer = AutoTokenizer.from_pretrained(path)\n","    device = device\n","    model.to(device)\n","\n","    results = []\n","    for i in trange(0, len(preds), batch_size):\n","        batch = [t for t in preds[i: i + batch_size]]\n","        inputs = tokenizer(batch, max_length=-1, padding=True, return_tensors='pt').to(device)\n","        with torch.no_grad():\n","            out = torch.softmax(model(**inputs).logits, -1)[:, 0].cpu().numpy()\n","            results.append(out)\n","    return np.concatenate(results)\n","\n","\n","def compute_metrics(eval_preds, tokenizer=None, print_results=False, batch_size=1, device='cuda'):\n","    preds, labels = eval_preds\n","    \n","    if tokenizer is not None:\n","        detokenized_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","        filtered_labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","        detokenized_labels = tokenizer.batch_decode(filtered_labels, skip_special_tokens=True)\n","    else:\n","        detokenized_preds = preds\n","        detokenized_labels = labels\n","\n","    results = {}\n","    results['toxic'] = get_toxicity(detokenized_preds, batch_size=batch_size, device=device)\n","    results['avg_toxic'] = sum(results['toxic']) / len(results['toxic'])\n","    cleanup()\n","\n","    results['bleu'] = get_sacrebleu(detokenized_labels, detokenized_preds) / 100\n","    cleanup()\n","\n","    results['fluency'] = get_fluency(detokenized_preds, batch_size=batch_size, device=device)\n","    results['avg_fluency'] = sum(results['fluency']) / len(results['fluency'])\n","    cleanup()\n","\n","    # count metrics\n","    results['joint'] = sum(results['toxic'] * results['bleu'] * results['fluency']) / len(preds)\n","    if print_results:\n","        print(\"--------------\")\n","        print(\"Metric   | Value\")\n","        print(\"--------------\")\n","        print(f\"toxic    | {results['avg_toxic']:.2f}\")\n","        print(f\"bleu (n) | {results['bleu']:.2f}\")\n","        print(f\"fluency  | {results['avg_fluency']:.2f}\")\n","        print(\"===============\")\n","        print(f\"Total    | {results['joint']:.2f}\")\n","        print(\"--------------\")\n","    return results"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T17:08:13.848222Z","iopub.status.busy":"2023-11-02T17:08:13.846932Z","iopub.status.idle":"2023-11-02T17:08:13.899277Z","shell.execute_reply":"2023-11-02T17:08:13.897951Z","shell.execute_reply.started":"2023-11-02T17:08:13.848179Z"},"trusted":true},"outputs":[],"source":["labels_list = [*test_subset['translation']]"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T17:08:21.122438Z","iopub.status.busy":"2023-11-02T17:08:21.121612Z","iopub.status.idle":"2023-11-02T17:09:11.326655Z","shell.execute_reply":"2023-11-02T17:09:11.325543Z","shell.execute_reply.started":"2023-11-02T17:08:21.122401Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:10<00:00,  7.60it/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"512751d83d7c45268388db8de3113868","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/79 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["--------------\n","Metric   | Value\n","--------------\n","toxic    | 0.69\n","bleu (n) | 0.21\n","fluency  | 0.91\n","===============\n","Total    | 0.13\n","--------------\n"]},{"data":{"text/plain":["{'toxic': array([0.9999584 , 0.99572176, 0.9999203 , ..., 0.9960717 , 0.9992476 ,\n","        0.01411206], dtype=float32),\n"," 'avg_toxic': 0.6933714614152908,\n"," 'bleu': 0.2101026067924932,\n"," 'fluency': array([0.98663205, 0.99079084, 0.9894153 , ..., 0.9943605 , 0.9873584 ,\n","        0.9846197 ], dtype=float32),\n"," 'avg_fluency': 0.9135820226486773,\n"," 'joint': 0.13385127502502436}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["compute_metrics((test_preds, labels_list), print_results=True, batch_size=64)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T17:06:30.856454Z","iopub.status.busy":"2023-11-02T17:06:30.855068Z","iopub.status.idle":"2023-11-02T17:06:30.929266Z","shell.execute_reply":"2023-11-02T17:06:30.928126Z","shell.execute_reply.started":"2023-11-02T17:06:30.856415Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Query</th>\n","      <th>Predictions</th>\n","      <th>Labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>If they say they don't want your help, you can...</td>\n","      <td>if they say they don't want your help, you can...</td>\n","      <td>if they say they don't need help, you can bet ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>When you go home, they'il call you \"Mr. No-Leg...</td>\n","      <td>When you get home, they'll call you \"Mr. No-Le...</td>\n","      <td>when you get home, they'll call you Mr.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Mrs. Sanders ain't goingfor that late shit no ...</td>\n","      <td>Mrs. Sanders won't be doing that late stuff an...</td>\n","      <td>Mrs. Sanders is not curious about being late.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I want every bloody one of them to hear.</td>\n","      <td>I want everyone to hear it.</td>\n","      <td>I want everyone to hear it.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>That shit hurt.</td>\n","      <td>that hurt.</td>\n","      <td>and that really hurts.</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>How do I know if I tell you what you need to k...</td>\n","      <td>I don't know if you'll tell me what I need to ...</td>\n","      <td>how do I know if I tell you what you want to k...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>This guy ain't going nowhere. Oh, shit!</td>\n","      <td>this guy's not going anywhere.</td>\n","      <td>this guy's not going anywhere.</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Fucked if I can remember.</td>\n","      <td>I can't remember.</td>\n","      <td>if only I could remember.</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>I fucked one stitch and he looks at me now as ...</td>\n","      <td>I'm sorry, but I'm not sure what you mean.</td>\n","      <td>I mess up a suture, and now he's looking at me...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Earth! Meet my lovely assistant... Tiny tits.</td>\n","      <td>earth, meet my assistant... tiny tits.</td>\n","      <td>please welcome my beautiful assistant, Maloprs...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               Query  \\\n","0  If they say they don't want your help, you can...   \n","1  When you go home, they'il call you \"Mr. No-Leg...   \n","2  Mrs. Sanders ain't goingfor that late shit no ...   \n","3           I want every bloody one of them to hear.   \n","4                                    That shit hurt.   \n","5  How do I know if I tell you what you need to k...   \n","6            This guy ain't going nowhere. Oh, shit!   \n","7                          Fucked if I can remember.   \n","8  I fucked one stitch and he looks at me now as ...   \n","9      Earth! Meet my lovely assistant... Tiny tits.   \n","\n","                                         Predictions  \\\n","0  if they say they don't want your help, you can...   \n","1  When you get home, they'll call you \"Mr. No-Le...   \n","2  Mrs. Sanders won't be doing that late stuff an...   \n","3                        I want everyone to hear it.   \n","4                                         that hurt.   \n","5  I don't know if you'll tell me what I need to ...   \n","6                     this guy's not going anywhere.   \n","7                                  I can't remember.   \n","8         I'm sorry, but I'm not sure what you mean.   \n","9             earth, meet my assistant... tiny tits.   \n","\n","                                              Labels  \n","0  if they say they don't need help, you can bet ...  \n","1            when you get home, they'll call you Mr.  \n","2      Mrs. Sanders is not curious about being late.  \n","3                        I want everyone to hear it.  \n","4                             and that really hurts.  \n","5  how do I know if I tell you what you want to k...  \n","6                     this guy's not going anywhere.  \n","7                          if only I could remember.  \n","8  I mess up a suture, and now he's looking at me...  \n","9  please welcome my beautiful assistant, Maloprs...  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","preds_dict = pd.DataFrame([[orig_i, pred_i, label_i] for orig_i, pred_i, label_i in zip(test_subset['reference'], test_preds, labels_list)], columns=['Query', 'Predictions', 'Labels'])\n","\n","preds_dict.head(10)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T17:06:30.930679Z","iopub.status.busy":"2023-11-02T17:06:30.930402Z","iopub.status.idle":"2023-11-02T17:06:30.985649Z","shell.execute_reply":"2023-11-02T17:06:30.984937Z","shell.execute_reply.started":"2023-11-02T17:06:30.930656Z"},"trusted":true},"outputs":[],"source":["preds_dict.to_csv('llama_test.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
